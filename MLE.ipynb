{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation\n",
    "\n",
    "This notebook illustrates maximum likelihood and how to estimate different standard errors (form the information matrix, the gradients and the \"sandwich\" approach).\n",
    "\n",
    "The application is very basic: estimate the mean and variance of a series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "printlnPs (generic function with 2 methods)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Dates, LinearAlgebra, DelimitedFiles, Statistics, Optim, ForwardDiff\n",
    "\n",
    "include(\"jlFiles/printmat.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx  = readdlm(\"Data/FFdSizePs.csv\",',',skipstart=1)\n",
    "x   = xx[:,2]                 #returns for the smallest size portfolio\n",
    "xx  = nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional Estimates\n",
    "\n",
    "of the mean $\\mu$ and the variance $\\sigma^2$.\n",
    "\n",
    "To compare with the MLE, we use $1/T$ in variance estimate, not $1/(T-1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traditional estimate and std:\n",
      "  estimate      std\n",
      "     0.042     0.010\n",
      "     0.840     0.013\n",
      "\n"
     ]
    }
   ],
   "source": [
    "T = length(x)\n",
    "\n",
    "(μ_trad,σ²_trad) = (mean(x),var(x,corrected=false))\n",
    "\n",
    "std_trad = sqrt.([σ²_trad,2*σ²_trad^2]/T)          #standard errors, textbook formulas\n",
    "\n",
    "println(\"Traditional estimate and std:\")\n",
    "println(\"  estimate      std\")\n",
    "printmat([[μ_trad,σ²_trad] std_trad])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point Estimates from ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Likelihood Function for Estimating the Parameters of a N(,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NormalLL (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function NormalLL(par::Vector,x)\n",
    "  (μ,σ²) = par\n",
    "  LLt    = -(1/2)*log(2*pi) .- (1/2)*log.(σ²) .- (1/2)*(x.-μ).^2/σ²  #vector, all x[t]\n",
    "  loss   = -sum(LLt)        #scalar, to minimize\n",
    "  return loss, LLt\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try the Likelihood Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log likelohood value at par0: -11155.385338928902\n"
     ]
    }
   ],
   "source": [
    "par0 = [0.0,1.0]                #initial parameter guess\n",
    "\n",
    "(loss,LLt) = NormalLL(par0,x)   #trying the log likelihood fn\n",
    "\n",
    "println(\"log likelohood value at par0: \",-loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize the Likelihood Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log-likelihood at point estimate: -11088.409\n",
      "\n",
      "Parameter estimates: \n",
      "   Traditional  MLE  \n",
      "     0.042     0.042\n",
      "     0.840     0.840\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Sol = optimize(par->NormalLL(par,x)[1],par0)  #minimize -sum(LLt)\n",
    "\n",
    "parHat = Optim.minimizer(Sol)                 #the optimal solution \n",
    "\n",
    "printlnPs(\"log-likelihood at point estimate: \",-Optim.minimum(Sol))\n",
    "\n",
    "println(\"\\nParameter estimates: \")\n",
    "println(\"   Traditional  MLE  \")\n",
    "printmat([[μ_trad,σ²_trad] parHat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Errors I: Information Matrix \n",
    "\n",
    "If the likelihood function is correctely specified, then MLE is typically asymptotically normally distributed as\n",
    "\n",
    "$\n",
    "\\sqrt{T}(\\hat{\\theta}-\\theta)  \\rightarrow^{d}N(0,V) \\: \\text{, where } \\: V=I(\\theta)^{-1}\\text{ with }\n",
    "$\n",
    "\n",
    "$\n",
    "I(\\theta) =-\\text{E}\\frac{\\partial^{2}\\ln L_t}{\\partial\\theta\\partial\\theta^{\\prime}}\n",
    "$\n",
    "\n",
    "where $I(\\theta)$ is the information matrix and $\\ln L_t$  is the contribution of period $t$ to the likelihood function.\n",
    "\n",
    "The code below calculates numerical derivatives.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard errors:\n",
      "   traditional Hessian \n",
      "     0.010     0.010\n",
      "     0.013     0.013\n",
      "\n"
     ]
    }
   ],
   "source": [
    "              #use the fact that E(derivative) = derivative(E)\n",
    "Ia       = -ForwardDiff.hessian(par->mean(NormalLL(par,x)[2]),parHat)\n",
    "Ia       = (Ia+Ia')/2         #to guarantee symmetry, fixes rounding errors\n",
    "vcv      = inv(Ia)/T\n",
    "std_hess = sqrt.(diag(vcv))\n",
    "\n",
    "println(\"standard errors:\")\n",
    "println(\"   traditional Hessian \")\n",
    "printmat([std_trad std_hess ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between information matrix and H_avg\n",
      "     0.000     0.000\n",
      "     0.000    -0.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#just for checking some of the previous calculations\n",
    "\n",
    "H = fill(NaN,(T,2,2))      #just to test, calculating Hessian for t and then averaging\n",
    "for t = 1:T\n",
    "    H[t,:,:] = -ForwardDiff.hessian(par->NormalLL(par,x[t])[2],parHat)    \n",
    "end\n",
    "\n",
    "H_avg = dropdims(mean(H,dims=1),dims=1)\n",
    "\n",
    "println(\"Difference between information matrix and H_avg\")\n",
    "printmat(Ia-H_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Errors II: Gradients and Sandwich\n",
    "\n",
    "Alternatively, we can use the outer product of the gradients to calculate the\n",
    "information matrix as\n",
    "\n",
    "$\n",
    "J(\\theta)=\\text{E}\\left[  \\frac{\\partial\\ln L_t}{\\partial\\theta\n",
    "}\\frac{\\partial\\ln L_t}{\\partial\\theta^{\\prime}}\\right]\n",
    "$\n",
    "\n",
    "We could also use the \"sandwich\" estimator\n",
    "\n",
    "$\n",
    "V=I(\\theta)^{-1}J(\\theta)I(\\theta)^{-1}.\n",
    "$\n",
    "\n",
    "When data is *not* iid $N($), then the three variance-covariance matrices may differ, and the sandwich approach is often the most robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Std from Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard errors\n",
      "    traditional Hessian  gradients\n",
      "     0.010     0.010     0.010\n",
      "     0.013     0.013     0.005\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LL_t_grad = ForwardDiff.jacobian(par->NormalLL(par,x)[2],parHat)   #T x 2 matrix\n",
    "J         = LL_t_grad'LL_t_grad/T\n",
    "vcv       = inv(J)/T\n",
    "std_grad  = sqrt.(diag(vcv))                          #std from gradients\n",
    "\n",
    "println(\"standard errors\")\n",
    "println(\"    traditional Hessian  gradients\")\n",
    "printmat([std_trad std_hess std_grad])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Std from Sandwich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard errors\n",
      "    traditional Hessian  gradients sandwich\n",
      "     0.010     0.010     0.010     0.010\n",
      "     0.013     0.013     0.005     0.036\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vcv       = inv(Ia) * J * inv(Ia)/T\n",
    "std_sandw = sqrt.(diag(vcv))                          #std from sandwich\n",
    "\n",
    "println(\"standard errors\")\n",
    "println(\"    traditional Hessian  gradients sandwich\")\n",
    "printmat([std_trad std_hess std_grad std_sandw])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try this: replace the data series `x` with simulated data from a $N()$ distribution. Do the different standard errors to get closer to each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 1.0.3",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
